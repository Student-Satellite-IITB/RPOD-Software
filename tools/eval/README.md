# VBN EVALUATOR

This directory contains scripts that evaluate the outputs of the VBN pipeline by comparing `results.txt` against ground truth:

- **SIM cases:** ground truth comes from `truth.txt` generated by `tools/sim/vbn_simulator.py`
- **HW cases:** ground truth is expected to be provided/derived from the hardware test setup (to be defined per HW dataset)

The evaluator is intended to be run on:
- a **single case folder** (one `CASE_*` directory), or
- a **cases root** containing multiple case folders.

Currently, only **Feature Detector (FD)** evaluation is implemented in `vbn_evaluator.py` (centroid error vs `truth.txt`).

---

## Expected case folder contents

A case folder typically contains:
- `image.png` — input image
- `truth.txt` — ground truth (SIM) *(may be absent for HW cases depending on dataset)*
- `results.txt` — outputs written by the C++ runner(s)
- *(optional)* `image_annotated_*.jpg` — debug overlays

---

## Usage

Run on a single case folder or a directory containing many case folders:

```bash
python3 tools/eval/vbn_evaluator.py --cases_root <path>
```

### Example
```bash
# Single case
python3 tools/eval/vbn_evaluator.py --cases_root tools/data/cases/sim/tmp_case

# Sweep directory (many cases)
python3 tools/eval/vbn_evaluator.py --cases_root tools/data/cases/sim/sweep_range_20251225_1851
```

### OUTPUTS
The evaluator writes an evaluation table (CSV) for batch runs (e.g., per-case centroid error metrics). 
- `fd_centroid_error_vs_range.png`
- `fd_summary.txt`
- `fd_eval.csv`

### Planned extensions:

- Static Pose Estimator (SPE) evaluation using rotation-error metrics from DCMs (not Euler angles)
- HW dataset evaluation conventions (case-level ground truth format, calibration metadata, etc.)